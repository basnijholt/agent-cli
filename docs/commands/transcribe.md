---
icon: lucide/mic
---

# transcribe

Transcribe audio from your microphone to text.

## Usage

```bash
agent-cli transcribe [OPTIONS]
```

## Description

This command:

1. Starts listening to your microphone immediately
2. Records your speech
3. When you press `Ctrl+C`, stops recording and finalizes transcription (Wyoming streams live; OpenAI uploads after stop)
4. Copies the transcribed text to your clipboard
5. Optionally uses an LLM to clean up the transcript

## Examples

```bash
# Basic transcription
agent-cli transcribe --input-device-index 1

# With LLM cleanup
agent-cli transcribe --input-device-index 1 --llm

# List available audio devices
agent-cli transcribe --list-devices

# Transcribe from a saved file (supports wav, mp3, m4a, ogg, flac, aac, webm)
agent-cli transcribe --from-file recording.wav

# Transcribe an MP3 file with OpenAI
agent-cli transcribe --from-file podcast.mp3 --asr-provider openai

# Transcribe an M4A voice memo with Gemini
agent-cli transcribe --from-file voice_memo.m4a --asr-provider gemini

# Re-transcribe most recent recording
agent-cli transcribe --last-recording 1

# Transcribe with speaker diarization (identifies different speakers)
agent-cli transcribe --diarize --hf-token YOUR_HF_TOKEN

# Diarization with JSON output format
agent-cli transcribe --diarize --diarize-format json --hf-token YOUR_HF_TOKEN

# Diarize a file with known number of speakers
agent-cli transcribe --from-file meeting.wav --diarize --min-speakers 2 --max-speakers 4 --hf-token YOUR_HF_TOKEN
```

## Supported Audio Formats

The `--from-file` option supports multiple audio formats:

| Provider | Supported Formats |
|----------|-------------------|
| OpenAI | mp3, mp4, mpeg, mpga, m4a, wav, webm |
| Gemini | wav, mp3, aiff, aac, ogg, flac, m4a |
| Wyoming | Any format (converted via ffmpeg) |

> [!NOTE]
> For non-WAV formats with the Wyoming provider, [ffmpeg](https://ffmpeg.org/) must be installed on your system.

## Options

<!-- CODE:START -->
<!-- from agent_cli.docs_gen import all_options_for_docs -->
<!-- print(all_options_for_docs("transcribe")) -->
<!-- CODE:END -->
<!-- OUTPUT:START -->
<!-- ⚠️ This content is auto-generated by `markdown-code-runner`. -->
### LLM Configuration

| Option | Default | Description |
|--------|---------|-------------|
| `--extra-instructions` | - | Additional instructions for the LLM to process the transcription. |
| `--llm/--no-llm` | `false` | Use an LLM to process the transcript. |

### Audio Recovery

| Option | Default | Description |
|--------|---------|-------------|
| `--from-file` | - | Transcribe audio from a file (supports wav, mp3, m4a, ogg, flac, aac, webm). Requires ffmpeg for non-WAV formats with Wyoming provider. |
| `--last-recording` | `0` | Transcribe a saved recording. Use 1 for most recent, 2 for second-to-last, etc. Use 0 to disable (default). |
| `--save-recording/--no-save-recording` | `true` | Save the audio recording to disk for recovery. |

### Provider Selection

| Option | Default | Description |
|--------|---------|-------------|
| `--asr-provider` | `wyoming` | The ASR provider to use ('wyoming', 'openai', 'gemini'). |
| `--llm-provider` | `ollama` | The LLM provider to use ('ollama', 'openai', 'gemini'). |

### Audio Input

| Option | Default | Description |
|--------|---------|-------------|
| `--input-device-index` | - | Index of the audio input device to use. |
| `--input-device-name` | - | Device name keywords for partial matching. |
| `--list-devices` | `false` | List available audio input and output devices and exit. |

### Audio Input: Wyoming

| Option | Default | Description |
|--------|---------|-------------|
| `--asr-wyoming-ip` | `localhost` | Wyoming ASR server IP address. |
| `--asr-wyoming-port` | `10300` | Wyoming ASR server port. |

### Audio Input: OpenAI-compatible

| Option | Default | Description |
|--------|---------|-------------|
| `--asr-openai-model` | `whisper-1` | The OpenAI model to use for ASR (transcription). |
| `--asr-openai-base-url` | - | Custom base URL for OpenAI-compatible ASR API (e.g., for custom Whisper server: http://localhost:9898). |
| `--asr-openai-prompt` | - | Custom prompt to guide transcription (optional). |

### Audio Input: Gemini

| Option | Default | Description |
|--------|---------|-------------|
| `--asr-gemini-model` | `gemini-3-flash-preview` | The Gemini model to use for ASR (transcription). |

### LLM: Ollama

| Option | Default | Description |
|--------|---------|-------------|
| `--llm-ollama-model` | `gemma3:4b` | The Ollama model to use. Default is gemma3:4b. |
| `--llm-ollama-host` | `http://localhost:11434` | The Ollama server host. Default is http://localhost:11434. |

### LLM: OpenAI-compatible

| Option | Default | Description |
|--------|---------|-------------|
| `--llm-openai-model` | `gpt-5-mini` | The OpenAI model to use for LLM tasks. |
| `--openai-api-key` | - | Your OpenAI API key. Can also be set with the OPENAI_API_KEY environment variable. |
| `--openai-base-url` | - | Custom base URL for OpenAI-compatible API (e.g., for llama-server: http://localhost:8080/v1). |

### LLM: Gemini

| Option | Default | Description |
|--------|---------|-------------|
| `--llm-gemini-model` | `gemini-3-flash-preview` | The Gemini model to use for LLM tasks. |
| `--gemini-api-key` | - | Your Gemini API key. Can also be set with the GEMINI_API_KEY environment variable. |

### Process Management

| Option | Default | Description |
|--------|---------|-------------|
| `--stop` | `false` | Stop any running background process. |
| `--status` | `false` | Check if a background process is running. |
| `--toggle` | `false` | Toggle the background process on/off. If the process is running, it will be stopped. If the process is not running, it will be started. |

### General Options

| Option | Default | Description |
|--------|---------|-------------|
| `--clipboard/--no-clipboard` | `true` | Copy result to clipboard. |
| `--log-level` | `WARNING` | Set logging level. |
| `--log-file` | - | Path to a file to write logs to. |
| `--quiet` | `false` | Suppress console output from rich. |
| `--config` | - | Path to a TOML configuration file. |
| `--print-args` | `false` | Print the command line arguments, including variables taken from the configuration file. |
| `--transcription-log` | - | Path to log transcription results with timestamps, hostname, model, and raw output. |

### Diarization

| Option | Default | Description |
|--------|---------|-------------|
| `--diarize/--no-diarize` | `false` | Enable speaker diarization (requires pyannote-audio). Install with: pip install agent-cli[diarization] |
| `--diarize-format` | `inline` | Output format for diarization ('inline' for [Speaker N]: text, 'json' for structured output). |
| `--hf-token` | - | HuggingFace token for pyannote models. Required for diarization. Accept license at: https://huggingface.co/pyannote/speaker-diarization-3.1 |
| `--min-speakers` | - | Minimum number of speakers (optional hint for diarization). |
| `--max-speakers` | - | Maximum number of speakers (optional hint for diarization). |


<!-- OUTPUT:END -->

## Workflow Integration

### Toggle Recording Hotkey

The `--toggle` flag is designed for hotkey integration:

```bash
# First press: starts recording
agent-cli transcribe --toggle --input-device-index 1

# Second press: stops recording and transcribes
agent-cli transcribe --toggle
```

### macOS Hotkey (skhd)

```
cmd + shift + r : /path/to/agent-cli transcribe --toggle --input-device-index 1
```

### Transcription Log

Log all transcriptions with timestamps:

```bash
agent-cli transcribe --transcription-log ~/.config/agent-cli/transcriptions.log
```

## Tips

- Use `--list-devices` to find your microphone's index
- Enable `--llm` for cleaner output with proper punctuation
- Use `--last-recording 1` to re-transcribe if you need to adjust settings

## Speaker Diarization

Speaker diarization identifies and labels different speakers in the transcript. This is useful for meeting recordings, interviews, or any multi-speaker audio.

### Requirements

1. **Install the diarization extra**:
   ```bash
   pip install agent-cli[diarization]
   # or with uv
   uv sync --extra diarization
   ```

2. **HuggingFace token**: The pyannote-audio models are gated. You need to:
   - Accept the license at [pyannote/speaker-diarization-3.1](https://huggingface.co/pyannote/speaker-diarization-3.1)
   - Get your token from [HuggingFace settings](https://huggingface.co/settings/tokens)
   - Provide it via `--hf-token` or the `HF_TOKEN` environment variable

### Output Formats

**Inline format** (default):
```
[SPEAKER_00]: Hello, how are you today?
[SPEAKER_01]: I'm doing well, thanks for asking!
[SPEAKER_00]: Great to hear.
```

**JSON format** (`--diarize-format json`):
```json
{
  "segments": [
    {"speaker": "SPEAKER_00", "start": 0.0, "end": 2.5, "text": "Hello, how are you today?"},
    {"speaker": "SPEAKER_01", "start": 2.7, "end": 4.1, "text": "I'm doing well, thanks for asking!"},
    {"speaker": "SPEAKER_00", "start": 4.3, "end": 5.2, "text": "Great to hear."}
  ]
}
```

### Speaker Hints

If you know how many speakers are in the recording, use `--min-speakers` and `--max-speakers` to improve accuracy:

```bash
# For a two-person interview
agent-cli transcribe --from-file interview.wav --diarize --min-speakers 2 --max-speakers 2 --hf-token YOUR_TOKEN
```

> [!NOTE]
> Diarization requires the audio file to be saved. When using live recording with `--diarize`, ensure `--save-recording` is enabled (it's enabled by default).
