---
icon: lucide/server
---

# server

Run ASR (Automatic Speech Recognition) and TTS (Text-to-Speech) servers.

## Usage

```bash
agent-cli server [COMMAND] [OPTIONS]
```

## Subcommands

| Command | Description |
|---------|-------------|
| `whisper` | Run local Whisper ASR server with GPU acceleration and TTL-based VRAM management |
| `tts` | Run local TTS server with Kokoro (GPU) or Piper (CPU) backends |
| `transcription-proxy` | Run proxy server that forwards to configured ASR providers |

---

## whisper

Run a local Whisper ASR server with automatic backend selection based on your platform:

- **macOS Apple Silicon** → [mlx-whisper](https://github.com/ml-explore/mlx-examples/tree/main/whisper) (Metal acceleration)
- **Linux/CUDA** → [faster-whisper](https://github.com/SYSTRAN/faster-whisper) (CTranslate2)

> [!NOTE]
> **Quick Start** - Get transcription working in 30 seconds:
> ```bash
> pip install "agent-cli[whisper]"
> agent-cli server whisper
> ```
> Server is now running at `http://localhost:10301`. Verify with `curl http://localhost:10301/health`.
>
> Apple Silicon MLX-only setup:
> ```bash
> pip install "agent-cli[whisper-mlx]"
> agent-cli server whisper --backend mlx
> ```
>
> Use it with any OpenAI-compatible client, or configure agent-cli to use it - see [Configuration](../configuration.md#using-local-whisper-server).

### Features

- **OpenAI-compatible API** at `/v1/audio/transcriptions` - drop-in replacement for OpenAI's Whisper API
- **Wyoming protocol** for [Home Assistant](https://www.home-assistant.io/) voice integration (Wyoming is the standard protocol for local voice services)
- **TTL-based VRAM management** - models unload after idle period, freeing GPU memory
- **Multiple models** - run different model sizes with independent TTLs
- **Background preloading** - downloads start at startup without blocking; use `--preload` to wait
- **Multi-platform support** - automatically uses the optimal backend for your hardware

### Usage

```bash
agent-cli server whisper [OPTIONS]
```

### Examples

```bash
# Run with default large-v3 model (5-minute TTL)
agent-cli server whisper

# Use smaller model with 10-minute TTL
agent-cli server whisper --model small --ttl 600

# Run multiple models (requests can specify which to use)
agent-cli server whisper --model large-v3 --model small --default-model large-v3

# Force CPU mode
agent-cli server whisper --device cpu

# Download model without starting server (requires faster-whisper)
agent-cli server whisper --model large-v3 --download-only

# Preload model at startup and wait until ready
agent-cli server whisper --preload
```

### Options

<!-- CODE:START -->
<!-- from agent_cli.docs_gen import all_options_for_docs -->
<!-- print(all_options_for_docs("server.whisper")) -->
<!-- CODE:END -->
<!-- OUTPUT:START -->
<!-- ⚠️ This content is auto-generated by `markdown-code-runner`. -->
### Options

| Option | Default | Description |
|--------|---------|-------------|
| `--model` | - | Model name(s) to load (can specify multiple) |
| `--default-model` | - | Default model when not specified in request |
| `--device` | `auto` | Device: auto, cuda, cuda:0, cpu |
| `--compute-type` | `auto` | Compute type: auto, float16, int8, int8_float16 |
| `--cache-dir` | - | Model cache directory |
| `--ttl` | `300` | Seconds before unloading idle model |
| `--preload` | `false` | Load model(s) at startup and wait for completion |
| `--host` | `0.0.0.0` | Host to bind the server to |
| `--port` | `10301` | HTTP API port |
| `--wyoming-port` | `10300` | Wyoming protocol port |
| `--no-wyoming` | `false` | Disable Wyoming server |
| `--download-only` | `false` | Download model(s) and exit without starting server |
| `--log-level` | `info` | Logging level: debug, info, warning, error |
| `--backend` | `auto` | Backend: auto (platform detection), faster-whisper, mlx |


<!-- OUTPUT:END -->

### API Endpoints

Once running, the server exposes:

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/v1/audio/transcriptions` | POST | OpenAI-compatible transcription |
| `/v1/audio/translations` | POST | OpenAI-compatible translation (to English) |
| `/v1/audio/transcriptions/stream` | WebSocket | Real-time streaming transcription |
| `/v1/model/unload` | POST | Manually unload a model from memory |
| `/health` | GET | Health check with model status |
| `/docs` | GET | Interactive API documentation |

### Using the API

#### curl Example

```bash
# Transcribe an audio file
curl -X POST http://localhost:10301/v1/audio/transcriptions \
  -F "file=@recording.wav" \
  -F "model=whisper-1"

# With language hint and verbose output
curl -X POST http://localhost:10301/v1/audio/transcriptions \
  -F "file=@recording.wav" \
  -F "model=whisper-1" \
  -F "language=en" \
  -F "response_format=verbose_json"

# Get SRT subtitles
curl -X POST http://localhost:10301/v1/audio/transcriptions \
  -F "file=@recording.wav" \
  -F "model=whisper-1" \
  -F "response_format=srt"
```

#### Python Example (OpenAI SDK)

```python
from openai import OpenAI

client = OpenAI(base_url="http://localhost:10301/v1", api_key="not-needed")

# Transcribe audio
with open("recording.wav", "rb") as f:
    transcript = client.audio.transcriptions.create(
        model="whisper-1",
        file=f,
    )
print(transcript.text)
```

### WebSocket Streaming Protocol

The `/v1/audio/transcriptions/stream` endpoint provides real-time streaming transcription.

#### Protocol

1. Connect to `ws://localhost:10301/v1/audio/transcriptions/stream?model=whisper-1`
2. Send binary audio chunks (16kHz, 16-bit, mono PCM)
3. Send `EOS` (3 bytes: `0x45 0x4F 0x53`) to signal end of audio
4. Receive JSON response with transcription

#### Message Format

**Server response:**
```json
{
  "type": "final",
  "text": "transcribed text here",
  "is_final": true,
  "language": "en",
  "duration": 3.5,
  "segments": [...]
}
```

**Error response:**
```json
{"type": "error", "message": "error description"}
```

### Model Selection Guide

| Model | Disk | VRAM | Speed | Accuracy | Use Case |
|-------|------|------|-------|----------|----------|
| `large-v3` | ~3GB | ~4GB | Slow | Best | Highest accuracy, batch processing |
| `medium` | ~1.5GB | ~2GB | Medium | Good | Balanced accuracy/speed |
| `small` | ~500MB | ~1GB | Fast | Fair | Real-time, lower VRAM |
| `tiny` | ~75MB | ~300MB | Fastest | Basic | Very limited VRAM, quick transcription |

> [!TIP]
> Use `--model small --model large-v3` to run both models. Clients can request either via the `model` parameter.

### Troubleshooting

| Issue | Solution |
|-------|----------|
| CUDA out of memory | Use `--device cpu` or smaller model (e.g., `--model small`) |
| Port already in use | Use `--port XXXX` to specify different port |
| Model download fails | Check network connection, or use `--download-only` first |
| Slow first request | Model is still downloading/loading. Use `--preload` to wait at startup |
| Wyoming not working | Ensure port 10300 is not blocked; check with `nc -zv localhost 10300` |

### Using with agent-cli Commands

The Whisper server is designed to work seamlessly with other agent-cli commands. See [Configuration: Local Whisper Server](../configuration.md#using-local-whisper-server) for setup instructions.

### Installation

Requires server deps and a backend:

```bash
# faster-whisper backend (also needed for --download-only)
pip install "agent-cli[whisper]"
# or
uv sync --extra whisper
```

#### macOS Apple Silicon

For optimal performance on M1/M2/M3/M4 Macs, install mlx-whisper:

```bash
pip install "agent-cli[whisper-mlx]"
```

The server will automatically detect and use the MLX backend when available.

#### Docker

Pre-built images are available from GitHub Container Registry:

```bash
# Run with GPU support
docker run -p 10300:10300 -p 10301:10301 --gpus all ghcr.io/basnijholt/agent-cli-whisper:latest-cuda

# Run CPU-only
docker run -p 10300:10300 -p 10301:10301 ghcr.io/basnijholt/agent-cli-whisper:latest-cpu
```

Or build from source using the [`whisper.Dockerfile`](https://github.com/basnijholt/agent-cli/blob/main/docker/whisper.Dockerfile):

```bash
# Build and run with GPU support
docker build -f docker/whisper.Dockerfile --target cuda -t agent-cli-whisper:cuda .
docker run -p 10300:10300 -p 10301:10301 --gpus all agent-cli-whisper:cuda

# Build and run CPU-only
docker build -f docker/whisper.Dockerfile --target cpu -t agent-cli-whisper:cpu .
docker run -p 10300:10300 -p 10301:10301 agent-cli-whisper:cpu
```

Or use [Docker Compose](https://github.com/basnijholt/agent-cli/blob/main/docker/docker-compose.whisper.yml):

```bash
# With GPU
docker compose -f docker/docker-compose.whisper.yml --profile cuda up

# CPU only
docker compose -f docker/docker-compose.whisper.yml --profile cpu up
```

Configure via environment variables:

| Variable | Default | Description |
|----------|---------|-------------|
| `WHISPER_MODEL` | `large-v3` | Model to load |
| `WHISPER_TTL` | `300` | Seconds before unloading idle model |
| `WHISPER_DEVICE` | `cuda`/`cpu` | Device (set by target) |
| `WHISPER_LOG_LEVEL` | `info` | Logging level |
| `WHISPER_EXTRA_ARGS` | - | Additional CLI arguments |

---

## tts

Run a local TTS (Text-to-Speech) server with two backend options:

- **[Kokoro](https://huggingface.co/hexgrad/Kokoro-82M)** - High-quality neural TTS with GPU acceleration (CUDA/MPS/CPU)
- **[Piper](https://github.com/rhasspy/piper)** - Fast, CPU-friendly ONNX-based synthesis

> [!NOTE]
> **Quick Start with Kokoro** (GPU-accelerated, auto-downloads from HuggingFace):
> ```bash
> pip install kokoro soundfile
> agent-cli server tts --backend kokoro
> ```
>
> **Quick Start with Piper** (CPU-friendly):
> ```bash
> pip install "agent-cli[tts]"
> agent-cli server tts --backend piper
> ```
>
> Server runs at `http://localhost:10401`. Verify with `curl http://localhost:10401/health`.

### Features

- **OpenAI-compatible API** at `/v1/audio/speech` - drop-in replacement for OpenAI's TTS API
- **Wyoming protocol** for [Home Assistant](https://www.home-assistant.io/) voice integration
- **TTL-based memory management** - models unload after idle period
- **Multiple backends** - Kokoro (GPU) or Piper (CPU)
- **Auto-download** - Models and voices download automatically on first use
- **Multiple voices** - Run different voices with independent TTLs

### Usage

```bash
agent-cli server tts [OPTIONS]
```

### Examples

```bash
# Run with Kokoro (auto-downloads model and voice from HuggingFace)
agent-cli server tts --backend kokoro

# Kokoro with specific voice
agent-cli server tts --backend kokoro --voice af_bella

# Run with Piper (CPU-friendly)
agent-cli server tts --backend piper

# Piper with specific voice and 10-minute TTL
agent-cli server tts --backend piper --model en_US-ryan-high --ttl 600

# Run multiple Piper voices
agent-cli server tts --backend piper --model en_US-lessac-medium --model en_GB-alan-medium

# Preload models at startup
agent-cli server tts --preload
```

### Options

<!-- CODE:START -->
<!-- from agent_cli.docs_gen import all_options_for_docs -->
<!-- print(all_options_for_docs("server.tts")) -->
<!-- CODE:END -->
<!-- OUTPUT:START -->
<!-- ⚠️ This content is auto-generated by `markdown-code-runner`. -->
### Options

| Option | Default | Description |
|--------|---------|-------------|
| `--model` | - | Model name(s) to load. Piper: 'en_US-lessac-medium'. Kokoro: 'kokoro' (auto-downloads) |
| `--default-model` | - | Default model when not specified in request |
| `--device` | `auto` | Device: auto, cpu, cuda, mps (Piper is CPU-only, Kokoro supports GPU) |
| `--cache-dir` | - | Model cache directory |
| `--ttl` | `300` | Seconds before unloading idle model |
| `--preload` | `false` | Load model(s) at startup and wait for completion |
| `--host` | `0.0.0.0` | Host to bind the server to |
| `--port` | `10401` | HTTP API port |
| `--wyoming-port` | `10400` | Wyoming protocol port |
| `--no-wyoming` | `false` | Disable Wyoming server |
| `--download-only` | `false` | Download model(s) and exit without starting server |
| `--log-level` | `info` | Logging level: debug, info, warning, error |
| `--backend` | `auto` | Backend: auto, piper, kokoro |
| `--voice` | - | Default voice for Kokoro (e.g., af_heart, af_bella) |


<!-- OUTPUT:END -->

### API Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/v1/audio/speech` | POST | OpenAI-compatible speech synthesis |
| `/v1/audio/speech/json` | POST | Alternative endpoint accepting JSON body |
| `/v1/voices` | GET | List available voices (models) |
| `/v1/model/unload` | POST | Manually unload a model from memory |
| `/health` | GET | Health check with model status |
| `/docs` | GET | Interactive API documentation |

### Using the API

#### curl Example

```bash
# Synthesize speech (form data)
curl -X POST http://localhost:10401/v1/audio/speech \
  -F "input=Hello, world!" \
  -F "model=tts-1" \
  -F "voice=alloy" \
  --output speech.wav

# With speed adjustment
curl -X POST http://localhost:10401/v1/audio/speech \
  -F "input=This is faster speech" \
  -F "voice=echo" \
  -F "speed=1.5" \
  --output fast.wav
```

#### Python Example (OpenAI SDK)

```python
from openai import OpenAI

client = OpenAI(base_url="http://localhost:10401/v1", api_key="not-needed")

# Synthesize speech
response = client.audio.speech.create(
    model="tts-1",
    voice="alloy",
    input="Hello, this is a test of the local TTS server.",
)
response.write_to_file("output.wav")
```

### Voice Selection

#### Kokoro Voices

Kokoro uses the `--voice` parameter. Voices auto-download from HuggingFace on first use.

The voice name prefix indicates accent: `af_` = American Female, `am_` = American Male, `bf_` = British Female, `bm_` = British Male.

| Voice | Accent | Gender | Notes |
|-------|--------|--------|-------|
| `af_heart` | American | Female | Default voice |
| `af_bella` | American | Female | |
| `af_nova` | American | Female | |
| `af_sky` | American | Female | |
| `am_adam` | American | Male | |
| `am_michael` | American | Male | |
| `bf_emma` | British | Female | |
| `bm_george` | British | Male | |

Browse all 30+ voices at [hexgrad/Kokoro-82M](https://huggingface.co/hexgrad/Kokoro-82M/tree/main/voices).

#### Piper Voices

With Piper, the **model name IS the voice**. Use `--model` to specify.

| Voice | Language | Quality |
|-------|----------|---------|
| `en_US-lessac-medium` | English (US) | Medium |
| `en_US-ryan-high` | English (US) | High |
| `en_GB-alan-medium` | English (UK) | Medium |
| `de_DE-thorsten-high` | German | High |
| `fr_FR-upmc-medium` | French | Medium |

Browse all voices at [rhasspy/piper](https://github.com/rhasspy/piper?tab=readme-ov-file#voices).

### Installation

#### Kokoro (GPU-accelerated)

```bash
pip install kokoro soundfile huggingface_hub
```

Kokoro requires PyTorch. For GPU acceleration:
- **CUDA**: Install PyTorch with CUDA support
- **Apple Silicon**: PyTorch automatically uses MPS

#### Piper (CPU-friendly)

```bash
pip install "agent-cli[tts]"
# or
uv sync --extra tts
```

---

## transcription-proxy

Run a proxy server that forwards transcription requests to your configured ASR provider (Wyoming, OpenAI, or Gemini).

This is useful for integrating agent-cli's transcription capabilities into other applications, such as iOS Shortcuts.

### Usage

```bash
agent-cli server transcription-proxy [OPTIONS]
```

### Examples

```bash
# Run on default port
agent-cli server transcription-proxy

# Custom port
agent-cli server transcription-proxy --port 8080

# Development mode with auto-reload
agent-cli server transcription-proxy --reload
```

### Options

<!-- CODE:START -->
<!-- from agent_cli.docs_gen import all_options_for_docs -->
<!-- print(all_options_for_docs("server.transcription-proxy")) -->
<!-- CODE:END -->
<!-- OUTPUT:START -->
<!-- ⚠️ This content is auto-generated by `markdown-code-runner`. -->
### Options

| Option | Default | Description |
|--------|---------|-------------|
| `--host` | `0.0.0.0` | Host to bind the server to |
| `--port` | `61337` | Port to bind the server to |
| `--reload` | `false` | Enable auto-reload for development |


<!-- OUTPUT:END -->

### API Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/transcribe` | POST | Transcribe audio file |
| `/health` | GET | Health check |
| `/docs` | GET | Interactive API documentation |

### Installation

Requires the `server` extra:

```bash
pip install "agent-cli[server]"
# or
uv sync --extra server
```

---

## Choosing Between Servers

| Use Case | Recommended |
|----------|-------------|
| Local GPU-accelerated transcription | `whisper` |
| High-quality GPU TTS | `tts --backend kokoro` |
| CPU-friendly TTS | `tts --backend piper` |
| Home Assistant voice integration | `whisper` + `tts` (both have Wyoming protocol) |
| iOS Shortcuts integration | `transcription-proxy` |
| Forwarding to cloud providers | `transcription-proxy` |
| Privacy-focused (no cloud) | `whisper` + `tts` |
| VRAM-constrained system | `whisper` (TTL unloading), `tts --backend piper` (CPU-only) |
